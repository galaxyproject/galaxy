<?xml version="1.0"?>
<!--
    Sample Object Store configuration file

    There should only be one root <object_store> tag, multiple are shown here to
    show different configuration options. Any object store can be used as
    backends to the distributed and hierarchical object stores (including
    distributed and hierarchical themselves).
-->
<!--
    Most of the Object Stores have <cache> option like:
    <cache path="database/object_store_cache" size="1000" cache_updated_data="True" />

    Here
    "path" - local path to store cached data,
    "size" - size of the cache in gigabytes.
    "cache_updated_data" - optional parameter that allows to control data
       is being sent directly
       to an object store without storing it in the cache.
       By default data is also copied to the cache (cache_updated_data="True").
-->


<!--
    Sample Disk Object Store

    This mirrors the default configuration if there is no object store
    configuration file. The default uses the values of file_path, new_file_path,
    and job_working_directory in galaxy.yml).
-->
<object_store type="disk" store_by="uuid">
    <files_dir path="database/objects"/>
    <extra_dir type="temp" path="database/tmp"/>
    <extra_dir type="job_work" path="database/jobs_directory"/>
</object_store>

<!--
    Sample Hierarchical Object Store with disk backends

    In the hierarchical object store, existing datasets will be searched for in
    backends in the order of the (0-indexed) "order" attribute on the backend
    tag, until the dataset is found. New datasets are always created in the
    first (order="0") backend.
-->
<!--
<object_store type="hierarchical">
    <backends>
        <backend id="new" type="disk" order="0">
            <files_dir path="/new-fs/galaxy/files"/>
            <extra_dir type="temp" path="/new-fs/galaxy/tmp"/>
            <extra_dir type="job_work" path="/new-fs/galaxy/jobs"/>
        </backend>
        <backend id="old" type="disk" order="1">
            <files_dir path="/old-fs/galaxy/files"/>
            <extra_dir type="temp" path="/old-fs/galaxy/tmp"/>
            <extra_dir type="job_work" path="/old-fs/galaxy/jobs"/>
        </backend>
    </backends>
</object_store>
-->

<!--
    Sample Distributed Object Store with disk backends

    In the distributed object store, existing datasets will be located by the
    `object_store_id` column in the `dataset` table of the Galaxy database,
    which corresponds to the `id` attribute on the backend tag. New datasets are
    created based on the "weight" attribute: a backend with weight "2" has a
    twice the chance of being (randomly) selected for new datasets as a backend
    with weight "1". A weight of "0" will still allow datasets in that backend
    to be read, but no new datasets will be written to that backend.

    In distributed and hierarchical world, you can choose that some backends are
    automatically unused whenever they become too full. Setting the maxpctfull
    attribute (on top level backends tag it behaves as a global default) enables
    this, or it can be applied to individual backends to override a global
    setting. This only applies to disk based backends and not remote object
    stores.

    By default, if a dataset should exist but its object_store_id is null, all
    backends will be searched until it is found. This is to aid in Galaxy
    servers moving from non-distributed to distributed object stores, but this
    behavior can be disabled by setting search_for_missing="false" on the top
    level backends tag.
-->
<!--
<object_store type="distributed">
    <backends>
        <backend id="new-big" type="disk" weight="3" maxpctfull="90">
            <files_dir path="/new-big-fs/galaxy/files"/>
            <extra_dir type="temp" path="/new-big-fs/galaxy/tmp"/>
            <extra_dir type="job_work" path="/new-big-fs/galaxy/jobs"/>
        </backend>
        <backend id="new-small" type="disk" weight="1" maxpctfull="90">
            <files_dir path="/new-small-fs/galaxy/files"/>
            <extra_dir type="temp" path="/new-small-fs/galaxy/tmp"/>
            <extra_dir type="job_work" path="/new-small-fs/galaxy/jobs"/>
        </backend>
        <backend id="old" type="disk" weight="0">
            <files_dir path="/old-fs/galaxy/files"/>
        </backend>
    </backends>
</object_store>
-->

<!--
    Sample Nested (Distributed in Hierarchical) Object Store

    These object stores support nesting object stores inside object stores. In
    this example, new data are created in the distributed object store, but old
    data will be searched for in a disk object store. This is useful if moving
    from non-distributed to distributed since you don't have to set
    `object_store_id` for old data in the database.

    In this example, new dataset creation is distributed evenly between two
    backends.
-->
<!--
<object_store type="hierarchical">
    <backends>
        <backend type="distributed" id="primary" order="0" maxpctfull="90">
            <backends>
                <backend id="new1" type="disk" weight="1" store_by="uuid">
                    <files_dir path="/new-fs/galaxy/files1"/>
                    <extra_dir type="temp" path="/new-fs/galaxy/tmp1"/>
                    <extra_dir type="job_work" path="/new-fs/galaxy/jobs1"/>
                </backend>
                <backend id="new2" type="disk" weight="1" store_by="uuid">
                    <files_dir path="/new-fs/galaxy/files2"/>
                    <extra_dir type="temp" path="/new-fs/galaxy/tmp2"/>
                    <extra_dir type="job_work" path="/new-fs/galaxy/jobs2"/>
                </backend>
            </backends>
        </backend>
        <backend type="disk" id="secondary" order="1">
            <files_dir path="/old-fs/galaxy/files"/>
        </backend>
    </backends>
</object_store>
-->

<!--
    Sample AWS S3 Object Store

    The "size" attribute of <cache> is in gigabytes.
-->
<!--
<object_store type="aws_s3">
     <auth access_key="...." secret_key="....." />
     <bucket name="unique_bucket_name_all_lowercase" use_reduced_redundancy="False" />
     <cache path="database/object_store_cache" size="1000" cache_updated_data="True" />
     <extra_dir type="job_work" path="database/job_working_directory_s3"/>
     <extra_dir type="temp" path="database/tmp_s3"/>
</object_store>
-->

<!--
    Sample Onedata Object Store

    Note for developers: you can easily set up a minimal, dockerized Onedata environment
    using the so-called "demo-mode": https://onedata.org/#/home/documentation/topic/stable/demo-mode

    Onedata specific options:
    //auth/@access_token -
        an access token suitable for data access (allowing calls to the Oneprovider REST API).

    //connection/@onezone_domain -
        the domain of the Onezone service (e.g. datahub.egi.eu), or its IP address for
        devel instances (see above).

    //connection/@disable_tls_certificate_validation - 
        Allows connection to Onedata servers that do not present trusted SSL certificates. 
        SHOULD NOT be used unless you really know what you are doing.

    //space/@name -
        the name of the Onedata space where the Galaxy data will be stored.

    //space/@path -
        the relative directory path in the space under which the Galaxy data will be stored.
        Optional, if not provided, the data will be stored in the root of space.
-->
<!--
<object_store type="onedata">
    <auth access_token="..." />
    <connection onezone_domain="datahub.egi.eu" disable_tls_certificate_validation="False"/>
    <space name="demo-space" path="galaxy-data" />
    <cache path="database/object_store_cache" size="1000" cache_updated_data="True" />
    <extra_dir type="job_work" path="database/job_working_directory_onedata"/>
    <extra_dir type="temp" path="database/tmp_onedata"/>
</object_store>
-->

<!--
    Sample iRODS Object Store
-->
<!--
<object_store type="irods">
    <auth username="rods" password="rods" />
    <resource name="demoResc" />
    <zone name="tempZone" />
    <connection host="localhost" port="1247" timeout="30" refresh_time="300" connection_pool_monitor_interval="3600"/>
    <cache path="database/object_store_cache_irods" size="1000" cache_updated_data="True" />
    <extra_dir type="job_work" path="database/job_working_directory_irods"/>
    <extra_dir type="temp" path="database/tmp_irods"/>
</object_store>
-->

<!--
    Sample non-AWS S3 Object Store (e.g. swift)

    The "size" attribute of <cache> is in gigabytes.
-->
<!--
<object_store type="generic_s3">
    <auth access_key="...." secret_key="....." />
    <bucket name="unique_bucket_name" use_reduced_redundancy="False" max_chunk_size="250"/>
    <connection host="" port="" is_secure="" conn_path="" multipart="True"/>
    <cache path="database/object_store_cache" size="1000" cache_updated_data="True" />
    <extra_dir type="job_work" path="database/job_working_directory_swift"/>
    <extra_dir type="temp" path="database/tmp_swift"/>
</object_store>
-->

<!--
    Sample Azure Object Store

    The "size" attribute of <cache> is in gigabytes.
-->
<!--
<object_store type="azure_blob">
    <auth account_name="..." account_key="...." />
    <container name="unique_container_name" max_chunk_size="250"/>
    <cache path="database/object_store_cache" size="100" cache_updated_data="True" />
    <extra_dir type="job_work" path="database/job_working_directory_azure"/>
    <extra_dir type="temp" path="database/tmp_azure"/>
</object_store>
-->

<!--
    Cloud ObjectStore: Amazon Simple Storage Service (S3)

    The "size" attribute of <cache> is in gigabytes.
-->
<!--
<object_store type="cloud" provider="aws" order="0">
    <auth access_key="..." secret_key="..." />
    <bucket name="..." use_reduced_redundancy="False" />
    <cache path="database/object_store_cache" size="100" cache_updated_data="True" />
    <extra_dir type="job_work" path="database/job_working_directory_s3"/>
    <extra_dir type="temp" path="database/tmp_s3"/>
</object_store>
-->

<!--
    Cloud ObjectStore: Microsoft Azure Blob Storage

    The "size" attribute of <cache> is in gigabytes.
-->
<!--
<object_store type="cloud" provider="azure" order="0">
    <auth subscription_id="..." client_id="..." secret="..." tenant="..." />
    <bucket name="..." use_reduced_redundancy="False" />
    <cache path="database/object_store_cache" size="100" cache_updated_data="True" />
    <extra_dir type="job_work" path="database/job_working_directory_azure"/>
    <extra_dir type="temp" path="database/tmp_azure"/>
</object_store>
-->

<!--
    Cloud ObjectStore: Google Compute Platform (GCP)

    The "size" attribute of <cache> is in gigabytes.
-->
<!--
<object_store type="cloud" provider="google" order="0">
    <auth credentials_file="..." />
    <bucket name="..." use_reduced_redundancy="False" />
    <cache path="database/object_store_cache" size="1000" cache_updated_data="True" />
    <extra_dir type="job_work" path="database/job_working_directory_gcp"/>
    <extra_dir type="temp" path="database/tmp_gcp"/>
</object_store>
-->


<!--
    User-Selectable Scratch Storage

    This distributed object store will default to a normal
    path on disk using the default quota but sets up a second
    path with user-private storage a larger quota and warns
    the user the disk is routinely cleaned. Relative speed
    and stability differences are communicated to the user
    using object store badges - as well as how data is backed up
    (in the default case) and not backed up for scratch storage.

    The admin is responsible for routinely cleaning that storage
    using Galaxy's admin scripts - this object store configuration
    just allows the user selection and communicates expectations
    to the user. Training related to Galaxy cleanup scripts can be
    found at:
    https://training.galaxyproject.org/training-material/topics/admin/tutorials/maintenance/slides.html.

    In this example, the scratch storage is marked as user-private
    using the private="true" attribute on the backend definition.
    This means it cannot be used in public datasets, shared between
    users, etc.. This is more example purposes - you may very well not
    want scratch storage to be defined as private as it prevents a lot
    of regular functionality and Galaxy handles regularly cleaned
    datasets fairly gracefully when the appropriate admin scripts
    are used.

    It is safe to just relabel the object store that a dataset belongs
    to if the underlying paths mapped to by the object stores are the
    same and the dataset has not been copied. To enable users to relocate
    datasets this way set the backends' `device` property to the same value.
-->
<!--
<object_store type="distributed">
    <backends>
        <backend id="default" allow_selection="true" type="disk" device="device1" weight="1" name="Default Galaxy Storage">
            <description>This is Galaxy's default object store - this disk is regularly backed up and all user's have a default quota of 200 GB.
            </description>
            <files_dir path="database/objects/deafult"/>
            <badges>
                <slower />
                <more_stable />
                <backed_up>Backed up to Galaxy's institutional long term tape drive nightly. More information about our tape drive can be found on our [Archive Tier Storage](https://www.msi.umn.edu/content/archive-tier-storage) page.</backed_up>
            </badges>
        </backend>
        <backend id="scratch" allow_selection="true" type="disk" device="device2" weight="0" name="Scratch Storage" private="true">
            <quota source="second_tier" />
            <description>This object store is connected to institutional scratch storage. This disk is not backed up and private to your user and datasets belonging to this storage will be automatically deleted after one month.
</description>
            <files_dir path="database/objects/temp"/>
            <badges>
                <faster />
                <less_stable />
                <not_backed_up />
                <short_term>The data stored here is purged after a month.</short_term>
            </badges>
        </backend>
    </backends>
</object_store>
-->

<!--
    User-Selectable Experimental Storage

    This distributed object store will default to a normal
    path on disk using the default quota but sets up a second
    path with more experimental storage (here iRODs) and a higher
    quota. The different backup strategies for normal disk and iRODs
    as well as their respective stability are communicated to the user
    using object store badges.
-->
<!--
<object_store type="distributed">
    <backends>
        <backend id="default" allow_selection="true" type="disk" weight="1" name="Default Galaxy Storage">
            <description>This is Galaxy's default object store - this disk is regularly backed up and all user's have a default quota of 200 GB.
            </description>
            <files_dir path="database/objects/deafult"/>
            <badges>
                <more_stable />
                <backed_up>Backed up to Galaxy's institutional long term tape drive nightly. More information about our tape drive can be found on our [Archive Tier Storage](https://www.msi.umn.edu/content/archive-tier-storage) page.</backed_up>
            </badges>
        </backend>
        <backend id="experimental" allow_selection="true" type="irods" weight="0" name="Experimental iRODS Storage">
            <quota source="irods_qutoa" />
            <description>This object store uses our experimental instituional iRODS service. This disk has larger quotas but is more experimental and expected job failure rates are higher.
</description>
            <auth username="rods" password="rods" />
            <resource name="demoResc" />
            <zone name="tempZone" />
            <connection host="localhost" port="1247" timeout="30" refresh_time="300" connection_pool_monitor_interval="3600"/>
            <cache path="database/object_store_cache_irods" size="1000" cache_updated_data="True" />
            <badges>
                <less_stable />
                <backed_up>This data is backed up using iRODs native hierarchal storage management mechanisms. The rules describing how data is stored and backed up in iRODS can be found in our institutional [iRODS documentation](https://irods.org/uploads/2018/Saum-SURFsara-Data_Archiving_in_iRODS-slides.pdf)</backed_up>
            </badges>
        </backend>
    </backends>
</object_store>
-->

<!--
    User-Selectable Storage - A Complex Institutional Example

    Huge chunks of text were stolen wholesale from MSI's data storage website
    (https://www.msi.umn.edu/content/data-storage). Large changes were made and adapted
    this for demonstration purposes - none of the text or policies or guarantees reflect
    actual current MSI or UMN policies.
-->
<!--
<object_store type="distributed">
    <backends>
        <backend id="high_performance" allow_selection="true" type="disk" weight="1" name="High Performance Storage">
            <description>All MSI researchers have access to a high-performance, high capacity primary storage platform. This system currently provides 3.5 PB (petabytes) of storage. The integrity of the data is protected by daily snapshots and tape backups. It has sustained read and write speeds of up to 25 GB/sec.

There is default access to this storage by any MSI group with an active account. Very large needs can be also met, but need to be approved by the MSI HPC Allocation Committee. More details are available on the [Storage Allocations](https://www.msi.umn.edu/content/storage-allocations) page.

More information about MSI Storage can be found [here](https://www.msi.umn.edu/content/data-storage).
</description>
            <files_dir path="/Users/jxc755/workspace/galaxy/database/objects/default"/>
            <badges>
                <faster />
                <more_stable />
                <backed_up>Backed up to MSI's long term tape drive nightly. More information about our tape drive can be found on our [Archive Tier Storage](https://www.msi.umn.edu/content/archive-tier-storage) page.</backed_up>
            </badges>
        </backend>
        <backend id="second" allow_selection="true" type="disk" weight="0" name="Second Tier Storage">
            <quota source="second_tier" />
            <description>MSI first added a Ceph object storage system in November 2014 as a second tier storage option. The system currently has around 10 PB of usable storage installed.

MSI's second tier storage is designed to address the growing need for resources that support data-intensive research. It is tightly integrated with other MSI storage and computing resources in order to support a wide variety of research data life cycles and data analysis workflows. In addition, this object storage platform offers new access modes, such as Amazon’s S3 (Simple Storage Service) interface, so that researchers can better manage their data and more seamlessly share data with other researchers whether or not the other researcher has an MSI account or is at the University of Minnesota.

More information about MSI Storage can be found [here](https://www.msi.umn.edu/content/data-storage).
</description>
            <files_dir path="/Users/jxc755/workspace/galaxy/database/objects/temp"/>
            <badges>
                <faster />
                <less_stable />
                <not_backed_up />
                <less_secure>MSI's enterprise level data security policies and montioring have not yet been integrated with Ceph storage.</less_secure>
                <short_term>The data stored here is purged after a month.</short_term>
            </badges>
        </backend>
        <backend id="experimental" allow_selection="true" type="disk" weight="0" name="Experimental Scratch" private="true">
            <quota enabled="false" />
            <description>MSI Ceph storage that is purged more aggressively (weekly instead of monthly) and so it only appropriate for short term methods development and such. The rapid deletion of stored data enables us to provide this storage without a quota.

More information about MSI Storage can be found [here](https://www.msi.umn.edu/content/data-storage).
            </description>
            <files_dir path="/Users/jxc755/workspace/galaxy/database/objects/temp"/>
            <badges>
                <faster />
                <less_stable />
                <not_backed_up />
                <less_secure>MSI's enterprise level data security policies and montioring have not yet been integrated with Ceph storage.</less_secure>
                <short_term>The data stored here is purged after a week.</short_term>
            </badges>
        </backend>
        <backend id="surfs" allow_selection="true" type="disk" weight="0" name="SURFS" private="true">
            <quota source="umn_surfs" />
            <description>Much of the data analysis conducted on MSI’s high-performance computing resources uses data gathered from UMN shared research facilities (SRFs). In recognition of the need for short to medium term storage for this data, MSI provides a service, Shared User Research Facilities Storage (SURFS), enabling SRFs to deliver data directly to MSI users. By providing a designated location for this data, MSI can focus data backup and other processes to these key datasets.  As part of this service, MSI will provide the storage of the data for one year from its delivery date.

It's expected that the consumers of these data sets will be responsible for discerning which data they may wish to keep past the 1-year term, and finding an appropriate place to keep it. There are several possible storage options both at MSI and the wider university. You can explore your options using OIT’s digital [storage options chooser tool](https://it.umn.edu/services-technologies/comparisons/select-digital-storage-options).

More information about MSI Storage can be found [here](https://www.msi.umn.edu/content/data-storage).</description>
            <badges>
                <slower />
                <more_secure>University of Minnesota data security analysist's have authorized this storage for the storage of human data.</more_secure>
                <more_stable />
                <backed_up />
            </badges>
        </backend>
    </backends>
</object_store>
-->
