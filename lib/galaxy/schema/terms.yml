unix:
  commandLine: |
    Galaxy typically runs a class of software called "command-line applications". A command
    line is essentially a string that describes what application should be run and what parameters
    it should be run with.

    More information on command lines can be found on [Wikipedia](https://en.wikipedia.org/wiki/Command-line_interface).
  exitCode: |
    An exit code is a numeric representation of how an application execution has completed. An exit code of value 0 typically indicates successful execution of an application, while other values indicate a failure.

    More information on exit codes can be found on [Wikipedia](https://en.wikipedia.org/wiki/Exit_status).
  stdout: |
    When an application is executed, most of its reporting, diagnostic, and logging messages are printed to a stream of
    data called the "standard output" (often abbreviated as stdout). Error messages are more typically
    printed to an alternative stream called "standard error".

    More information on standard output can be found on [Wikipedia](https://en.wikipedia.org/wiki/Standard_streams).
  stderr: |
    When an application is executed, most of its error messages are typically printed to a stream of
    data called the "standard error" (often abbreviated as stderr). Other reporting, diagnostic, and
    logging messages are typically printed to an alternative stream called "standard output".

    More information on standard error can be found on [Wikipedia](https://en.wikipedia.org/wiki/Standard_streams).
  traceback: |
    A "traceback" is also often called a "stack trace". This is typically used to describe what is
    happening at various levels of an application at an instant in time and can sometimes be used to
    debug what is wrong with the execution of an application.

    More information on stack traces can be found on [Wikipedia](https://en.wikipedia.org/wiki/Stack_trace).
programming:
  python:
    regex: |
      Regular expressions are patterns used to match character combinations in strings. This input accepts Python-style regular expressions, find more information about these in [this Python for Biologists tutorial](https://pythonforbiologists.com/tutorial/regex.html).

      The website [regex101](https://regex101.com/) is a useful playground to explore regular expressions (be sure to enable "Python" as your flavor) and language models such as ChatGPT can help interactively build up and explain regular expressions from natural language prompts or examples.
galaxy:
  dataFetch:
    dbkey: |
      This identifier is used to associate datasets with specific reference genomes. If set, the dbkey
      is a string that represents the genome assembly, such as "hg19" for human genome version 19 or "mm10"
      for mouse genome version 10. In other parts of of the API this is referred to as the "genome_build".
      The Galaxy user interface also refers to this as "build" or "custom build". The value "?" is used to
      indicate that the dataset does not have a dbkey set.
    info: |
      Free text field that can be used to store arbitrary information about the dataset. This used to be prominently
      displayed in the Galaxy user interface, but now is largely unused.
    ext: |
      The file extension of the dataset. This is shorthand description of the datatype corresponding to this dataset.
      The default "auto" is used to indicate that the datatype should be automatically determined by Galaxy based on
      the contents of the file.
    space_to_tab: |
      This is a boolean value that indicates whether the spaces in the dataset contents should be converted to tabs.
      This should typically be set to false for most applications, but sometimes when pasting data into the Galaxy
      user interface, it is useful to set this to true to ensure that the data is converted to a tabular format
      correctly.
    to_posix_lines: |
      This is a boolean value that indicates whether the line endings in the dataset should be converted to POSIX
      line endings (LF). The Galaxy user interface will typically set this to true so that all datasets default
      to having POSIX line endings as most tools and workflows expect. The actual upload API will default this to false
      though assuming the API user is more likely to be want to be precise about file handling details.
    auto_decompress: |
      This is a boolean value that indicates whether the dataset should be automatically decompressed if it is
      compressed. If set to true, Galaxy will attempt to decompress the dataset if it is compressed and it is not
      explicitly set to a compressed datatype.
    deferred: |
      This is a boolean value that indicates whether the dataset is deferred. Deferred datasets are not
      immediately ingested into Galaxy on data import and may lack some metadata. Given open bugs with deferred
      datasets, most datasets should not be deferred unless you are sure you want to use this feature.
    tags: |
      Tags are a way to categorize datasets in Galaxy. They are free-form text strings that can be used to
      group datasets together. Tags can be used to filter datasets in the Galaxy user interface and can be
      used to search for datasets in the Galaxy API.
    MD5: |
      The MD5 checksum of the dataset. This is a hash of the dataset contents that can be used to verify the
      integrity of the dataset. More information on MD5 checksums can be found on [Wikipedia](https://en.wikipedia.org/wiki/MD5).
    SHA1: |
      The SHA1 checksum of the dataset. This is a hash of the dataset contents that can be used to verify the
      integrity of the dataset. More information on SHA1 checksums can be found on [Wikipedia](https://en.wikipedia.org/wiki/SHA-1).
    SHA256: |
      The SHA-256 checksum of the dataset. This is a hash of the dataset contents that can be used to verify the
      integrity of the dataset. More information on SHA-256 checksums can be found on [Wikipedia](https://en.wikipedia.org/wiki/SHA-2).
    SHA512: |
      The SHA-512 checksum of the dataset. This is a hash of the dataset contents that can be used to verify the
      integrity of the dataset. More information on SHA-512 checksums can be found on [Wikipedia](https://en.wikipedia.org/wiki/SHA-2).
    check_content: |
      This should not be included in the API request - it is a Galaxy-wide configuration setting that indicates whether
      Galaxy should check for various kinds of malicious contents in uploaded/imported datasets.
    purge_source: |
      If importing from file sources, purge the source file after the dataset has been created. Should generally
      default to false, but various Galaxy configuration options and job running modes may swap this.
    paste_content: |
      This is the text of the content to import if the 'src' of the item is 'pasted'.
    targets: |
      This groups data import requests into sets of elements or items that should be imported together. Each
      target should have a a set of items/elements as well as a 'destination'. The ``destination`` of the target controls where
      the data is imported to - for instances as HDAs into a history (``{"type": "hdas"}``) or as a collection
      (``{"type": "hdca"}``).
    sources:
      url: |
        This source type indicates the item should be imported from a Galaxy URI. This could be a traditional HTTP/HTTPS URL (e.g.
        something that starts with https:// or http://) or any other kind of URI this Galaxy is configured to
        handle.
      pasted: |
        This source type indicates the item should be just the literal contents embedded in the request using the
        the ``paste_content`` field.        
      files: |
        This source type indicates the item should be imported from a file that is uploaded to Galaxy with the
        the request.
      path: |
        This source type indicates the item should be imported from a file that is already present on the Galaxy
        server. This functionality is typically restricts to administrators of the Galaxy instance.
      ftp_import: |
        This source type indicates the item should be imported from the user's configured "FTP directory".
      server_dir: |
        This source type indicates the item should be imported from the user's library import folder.

    destinations:
      hdas: |
        This destination type indicates the items should be imported as individual datasets into a history.
      hdca: |
        This destination type indicates the items should be imported as a collection into a history.
      library_folder: |
        This destination type indicates the items should be imported as a library folder.
      library: |
        This destination type indicates the items should be imported as a library.
  collections:
    flatList: |
      A flat list is just a simple dataset collection of type ``list`` that contains only datasets and not
      other collections.
    mapOver: |
      When a tool consumes a dataset but is run with a collection, the collection *maps over* the collection.
      This means instead of just running the tool once - the tool will be run once for each element of the
      provided collection. Additionally, the outputs of the tool will be collected into a collection that
      matches the structure of the provided collection. This matching structure means the output collections
      will have the same element identifiers as the provided collection and they will appear in the same order.

      It is easiest to visualize "mapping over" a collection in the context of a tool that consumes a dataset
      and produces a dataset, but the semantics apply rather naturally to tools that consume collections or
      produce collections as well.
      
      For instance, consider a tool that consumes a ``paired`` collection and produces an output dataset.
      If a list of paired collections (collection type ``list:paired``) is passed to the tool - it will
      produce a flat list (collection type ``list``) of output datasets with the same number of elements
      in the same order as the provided list of ``paired`` collections.

      In the case of outputs, consider a tool that takes in a dataset and produces a flat list. If this tool
      is run over a flat list of datasets - that list will be "mapped over" and each element will produce a list.
      These lists will be gathered together in a nested list structure (collection type ``list:list``) where
      the outer element count and structure matches that of the input and the inner list for each of those
      is just the outputs of the tool for the corresponding element of the input.
    collectionBuilder:
      hideOriginalElements: |
        Toggling this on means that the original history items that will become a part of the collection
        will be hidden from the history panel (they will still be searchable via the 'visible: false' filter).
      filteredExtensions: |
        The history is filtered for the formats that are required for this collection. You might see some
        items with other formats since those can still be valid inputs via implicit conversion.
      requiredUploadExtensions: |
        The formats that are required for this collection. The files you upload will be assumed to have
        these formats. In the case of more than one format, you can select a specific format for
        each individual file above. If there is only one format, Galaxy will attempt to set that as the
        format for each file.
      whyHomogenousCollections: |
        Dataset collections are designed to streamline the analysis of large numbers of datasets by grouping
        them together into a single, manageable entity. Unlike generic folders on your computer, which can hold
        any mix of file types, dataset collections are specifically intended to be homogenous. This homogeneity
        is crucial for consistency in processing. Homogeneous datasets ensure that each dataset in the collection
        can be processed uniformly with the same tools and workflows. This eliminates the need for individual
        adjustments, which can be time-consuming and prone to error. Most tools and workflows in Galaxy are designed
        to operate on collections of similar data types. Homogeneous collections allow these tools to operate
        uniformly over the collection.

  jobs:
    metrics:
      cores: |
        This is how many [cores](https://en.wikipedia.org/wiki/Central_processing_unit) (or distinct central processing units (CPUs)) are
        allocated to run the job for the tool. This value is generally configured for the tool by the Galaxy administrator. This value
        does not guarantee how many cores the job actually used - the job may have used more or less based on how Galaxy is configured
        and how the tool is programmed.

      walltime: |
        This is an estimate of the length of time the job ran, created by recording the start and stop times of the job in the job script
        created for the tool execution and subtracting these values.

      allocated_core_time: |
        This is the number of cores Galaxy believes is allocated for the job times the estimated walltime for the job. This can be thought
        of as scaling the runtime/walltime metric by the number of cores allocated. When purchasing compute time per core hour or consuming
        compute time from a compute center's allocation, this is likely to be a more interesting and useful number than the walltime.

    states:
      # upload, waiting, failed, paused, deleting, deleted, stop, stopped, skipped.
      ok: |
        This state indicates the job completed normally and Galaxy did not detect any errors with execution. 
      new: |
        This state indicates the job is ready to be run. Typically this means a Galaxy job handler will schedule
        the job and place it into a queued state on its next iteration.
      queued: |
        This state indicates that Galaxy has scheduled the job and some external resource has placed it in a queue.
        Typically, once the job has reached the front of that queue it will be executed and transitioned to the
        'running' state.
      error: |
        This state indicates that Galaxy has attempted to execute this job and there was some issue.
      failed: |
        This state indicates that Galaxy has attempted to execute this job and it completed but Galaxy
        detected some issue with the execution.
      running: |
        This state indicates that Galaxy is currently running this job. After the job is complete, it will
        likely be transitioned to a 'failed' or 'ok' state.
      paused: |
        This state indicates that Galaxy has paused attempts to execute this job. This can be because
        upstream jobs have failed and so inputs will not become available or because job or quota limits
        have been reached.

  invocations:
    states:
      scheduled: |
        This state indicates the workflow invocation has had all of its jobs scheduled. This means all the
        datasets are likely created and Galaxy has created the stubs for the jobs in the workflow. *The jobs
        themselves might not have been queued or running.*

      ready: |
        This state indicates the workflow invocation is ready for additional scheduling steps. This means
        the workflow will likely result in additional datasets and jobs being created over time.

      new: |
        This state indicates the workflow invocation has been queued up but no datasets or jobs have been
        created.

      cancelled: |
        This state indicates the workflow invocation has been cancelled and new jobs or datasets won't
        be created for this workflow. Most cancellations are caused by user interactions. If problems
        scheduling the workflow cause a failure that cannot be recovered from, the state will be failed
        instead of cancelled.

      cancelling: |
        This state indicates the workflow invocation will be cancelled shortly by Galaxy.

      failed: |
        This state indicates there was a problem scheduling the workflow invocation. No additional datasets
        or jobs will be created for this workflow invocation.
  upload:
    galaxyUploadUtil: |
      A utility for uploading files to a Galaxy server from the command line. Use ``galaxy-upload`` to upload
      file(s) to a Galaxy server, and ``galaxy-history-search``, a helper utility to find Galaxy histories
      to pass to the ``galaxy-upload`` command.
    ruleBased: |
      Galaxy can bulk import lists & tables of URLs into datasets or collections using reproducible rules.
  
  datasets:
    formatVsDatatypeVsExtension: |
      In Galaxy, the terms "datatype," "format," and "extension" are used to describe the nature of a dataset,
      each with specific meanings.
      
      The ``datatype`` defines how a dataset is interpreted, validated, and processed, such as "fastqsanger", "bam",
      or "vcf". Each datatype corresponds to a Python class that specifies how Galaxy should handle the file.
      
      The ``format`` is the user-facing label for the dataset's datatype, appearing in the Galaxy user interface
      to indicate the structure and content expectations of the dataset.
      
      The ``extension`` is a string associated with each datatype, stored in Galaxy’s database to identify the
      dataset’s datatype. This extension can match a file’s actual disk suffix (e.g., ".vcf") but may also be more
      specialized (e.g., "fastqsanger"). Understanding these distinctions ensures that datasets are processed
      correctly within Galaxy, maintaining the integrity and reproducibility of analyses.

  workflows:
    runtimeSettings:
      sendToNewHistory:
        Enabling this option will create a new history for the workflow invocation. This is useful if you want to
        keep the results of the workflow separate from your current history. The inputs to the workflow will also be
        copied to the new history.
      useCachedJobs:
        Enabling this option will use cached jobs for the workflow invocation. This is useful if you want to
        reuse the results of a previous job for the same input data.
      splitObjectStore:
        Enabling this option allows you to select and send outputs and intermediate datasets to different storage
        locations independently of each other.

